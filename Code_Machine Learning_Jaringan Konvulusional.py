# -*- coding: utf-8 -*-
"""Kelompok5_MachineLearning_JARINGAN KONVOLUSIONAL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VA08D_JhkNBLbfcwl0PL9HjzpS1RF_cc

*   Author : Kelompok 5
*   Affiliation : SAINS DATA ITERA
*   Date : 09 Mei 2023
*   Classes : Machine Learning
*   Topic : Jaringan Konvolusional


**Tugas Kelompok Paraktikum Modul 5**
"""

from google.colab import drive
drive.mount('/content/gdrive/')

"""***LANGKAH 1 : USING LIBRARY WANT EXECUTION***"""

import os
import shutil
import keras
import random
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
keras = tf.keras
from tensorflow.python.training.input import batch
from tensorflow.python.ops.batch_ops import batch
from tensorflow._api.v2 import image

"""#***DATASET SAWI***

***LANGKAH 2 : IMPORT & SPLIT DATA***
"""

def loading_data():
    dirname = '/content/gdrive/MyDrive/SAWI/'
    dirtype = ['train/', 'valid/']
    dirclass = ['Data Sawi Ada Hama', 'Data Sawi Tanpa Hama/']

    x_train = []
    y_train = []
    x_test = []
    y_test = []

    for typ in dirtype :
        for cls in dirclass :
            for i in os.listdir(dirname+typ+cls):
                if typ == 'train/':
                    x_train.append(dirname+'train/'+cls+i)
                    y_train.append(cls[:-1])
                else:
                    x_test.append(dirname+'valid/'+cls+i)
                    y_test.append(cls[:-1])
    return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)

#Run loading data() function
x_train, y_train, x_test, y_test = loading_data()

"""***LANGKAH 3 : DATA PREPROCESSING***"""

#DATA Preprocessing
#Processing image files into numpy array
def process_label(label):
    label = [i == unique_label for i in label]
    label = np.array(label).astype(int)
    return label

def processImage(path):
    image = tf.io.read_file(path)
    image = tf.image.decode_jpeg(image,channels =3)
    image = tf.image.convert_image_dtype(image, tf.float32)
    image = tf.image.resize(image, size=[224,224])
    return image

#Create Batch Data of NumPy Array
def pairData(image, label):
    return processImage(image), label

def batchData(image, label = None, for_valid= False, for_test = False):
    if for_test:
        data = tf.data.Dataset.from_tensor_slices((image))
        batch = data.map(processImage).batch(32)
        return batch
    elif for_valid:
        data = tf.data.Dataset.from_tensor_slices((tf.constant(image),tf.constant(label)))
        batch = data.map(pairData).batch(32)
        return batch
    else:
        data = tf.data.Dataset.from_tensor_slices((tf.constant(image),tf.constant(label)))
        data = data.shuffle(buffer_size=len(image))
        batch = data.map(pairData).batch(32)
        return batch

unique_label = np.unique(y_test)
y_test = process_label(y_test)
y_train = process_label(y_train)

train_data = batchData(x_train, y_train)
valid_data = batchData(x_test, y_test, for_valid = True)

x_train, y_train, x_test, y_test = loading_data()
x_train, x_test
y_train, y_test

"""***LANGKAH 4 : MEMBANGUN NEURAL NETWORK***"""

#Create model structure
model = keras.Sequential([
    #Input Layer
    keras.layers.Conv2D(input_shape = (224,224,3), filters=32, kernel_size=(3,3), activation = 'relu'),
    keras.layers.MaxPooling2D(),

    #Hidden Layer
    keras.layers.Conv2D(input_shape = (224,224,3), filters = 64, kernel_size=(3,3), activation = 'relu'),
    keras.layers.MaxPooling2D(),
    keras.layers.Conv2D(input_shape = (224,224,3), filters = 128, kernel_size=(3,3), activation = 'relu'),
    keras.layers.MaxPooling2D(),
    keras.layers.Flatten(),
    keras.layers.Dense(128),
    keras.layers.Activation('relu'),

    #Output Layer
    keras.layers.Dense(2),
    keras.layers.Activation('softmax')
])

# Compile model
model.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics = ['acc'])

model.summary()

"""Kesimpulan yang dapat diambil dari output diatas adalah bahwa itu adalah output dari model yang memiliki arsitektur Convolutional Neural Network (CNN) untuk klasifikasi gambar. Model ini memiliki 3 layer konvolusi dengan masing-masing layer dilanjutkan dengan MaxPooling layer untuk mengurangi dimensi input. Kemudian, output dari layer konvolusi terakhir dikembangkan ke dalam bentuk vektor dengan menggunakan Flatten layer. Vektor ini kemudian dilanjutkan ke dalam 2 layer Dense dengan masing-masing memiliki 128 dan 2 unit. Pada akhirnya, aktivasi softmax digunakan pada layer output untuk memberikan probabilitas kelas pada gambar masukan. Total parameter model ini adalah 11.169.218 dan seluruhnya trainable.

***LANGKAH 5 : MODEL PELATIHAN***
"""

#buat panggilan balik penghentian awal
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

# Simpan riwayat model menjadi variabel
history2 = model.fit(train_data,validation_data = valid_data, validation_freq=1, epochs = 5, callbacks = [early_stopping], verbose= 1)

#Plot model training history
def plot_history():
  plt.plot(history.history['acc'], label='acc')
  plt.plot(history.history['val_acc'],label='val_acc')
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'],label='val_loss')
  plt.legend()
  plt.title('Training History')
  plt.xlabel('epoch')
  plt.ylabel('value')
  plt.tight_layout()
  plt.grid(True)
  plt.savefig('Training_history.jpg')
  plt.show()

plot_history()

"""Dari hasil pelatihan model diatas yang dilakukan selama 5 epoch, diperoleh akurasi training sebesar 95.13% dan akurasi validasi sebesar 90.45%. Terdapat perbedaan antara akurasi training dan validasi, namun selisihnya tidak terlalu signifikan sehingga dapat dianggap model tersebut tidak mengalami overfitting. Dalam pelatihan tersebut, loss pada training set dan validation set juga menurun secara bertahap. Namun, pada epoch ke-2 terjadi kenaikan nilai loss pada validation set, namun nilai tersebut kembali menurun pada epoch selanjutnya. Oleh karena itu, dapat disimpulkan bahwa model telah berhasil dilatih dengan baik dan dapat digunakan untuk memprediksi apakah seseorang menggunakan masker atau tidak.

***MODEL LAIN***
"""

# Create mode structure
import keras.utils
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from tensorflow.keras import optimizers
model2 = keras.Sequential([
    # Input layer
    keras.layers.Conv2D(input_shape=(224,224,3),filters=32,kernel_size=(3,3),activation='relu'),
    keras.layers.MaxPooling2D(),

    # Hidden Layer
    keras.layers.Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),activation='relu'),
    keras.layers.MaxPooling2D(),
    keras.layers.Conv2D(input_shape=(224,224,3),filters=128,kernel_size=(3,3),activation='relu'),
    keras.layers.MaxPooling2D(),
    keras.layers.Flatten(),
    keras.layers.Dense(128),
    keras.layers.Activation('relu'),

    # Output layer
    keras.layers.Dense(2),
    keras.layers.Activation('sigmoid')
])

# Compile model
model2.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.SGD(),metrics=['acc'])

model2.summary()

"""Model sequential yang digunakan memiliki arsitektur yang sama dengan model sebelumnya dengan jumlah parameter yang sama. Dalam pelatihan model, diperoleh akurasi validasi yang lebih tinggi daripada akurasi training, sehingga model cenderung mengalami overfitting. Selain itu, terdapat perbedaan kinerja antara data training dan data validasi pada epoch ke-2, yang menunjukkan adanya masalah pada pelatihan model."""

# buat panggilan bail penghentian awal
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',patience=3)

# simpan riwayat model menjadi variabel
history2 = model2.fit(train_data,validation_data = valid_data,validation_freq=1,epochs=5,callbacks=[early_stopping],verbose = 1)

"""***LANGKAH 6 : DEMONSTRASI***"""

def predict(unseen_image = []):
    test_data = batchData(unseen_image,for_test=True)
    prediction = model.predict(test_data)
    for image, pred in zip(unseen_image,prediction):
        fig, axes = plt.subplots(nrows=1,ncols=2)
        axes[0].imshow(processImage(image))
        axes[0].axis(False)
        axes[0].set_title('Actual Image')
        axes[1].bar([0,1],pred)
        axes[1].set_xticks([0,1])
        axes[1].set_xticklabels(['Data Sawi Ada Hama', 'Data Sawi Tanpa Hama'])
        axes[1].set_title('Prediction Probability')
        plt.savefig(f'/content/drive/MyDrive/output/prediction_test_{image.split("/")[-1].split(".")[0]}.jpg')
        plt.show()

predict(list(map(lambda x : f'/content/drive/MyDrive/test_sawi/{x}',os.listdir('/content/drive/MyDrive/test_sawi'))))

"""#***DATASET FACEMASK***

***LANGKAH 1 : IMPORT & SPLIT DATA***
"""

import os
import shutil

# mengakses dataset awal
data_dir = '/content/drive/MyDrive/data masker/data/'

# membuat informasi class
all_class = {'train':{},'valid':{}}
for cl in os.listdir(data_dir):
    items = os.listdir(f'{data_dir}/{cl}')
    test_count = int(0.2*(len(items)))
    all_class['train'][cl] = items[test_count:]
    all_class['valid'][cl] = items[:test_count]

# memindahkan file dataset awal ke folder baru
for category,cl in all_class.items():
    for sub_class,item in cl.items():
        for i in item:
            print(f'{category}/{sub_class}/{i}')
            des_path = f'dataset/{category}/{sub_class}/'
            os.makedirs(des_path, exist_ok = True)
            shutil.move(f'{data_dir}/{sub_class}/{i}',des_path)

def loading_data():
    dirname = '/content/dataset/'
    dirtype = ['train/', 'valid/']
    dirclass = ['with_mask/', 'without_mask/']

    x_train = []
    y_train = []
    x_test = []
    y_test = []

    for typ in dirtype:
        for cls in dirtype:
            for cls in dirclass:
                for i in os.listdir(dirname+typ+cls):
                    if typ == 'train/':
                      x_train.append(dirname+'train/'+cls+i)
                      y_train.append(cls[:-1])
                    else:
                        x_test.append(dirname+'valid/'+cls+i)
                        y_test.append(cls[:-1])
    return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)

# Run loading_data() function
x_train, y_train, x_test, y_test = loading_data()

"""***LANGKAH 2 : DATA PREPROCESSING***"""

# Processing image
def process_label(label):
    label = [i == unique_label for i in label]
    label = np.array(label).astype(int)
    return label

def processImage(path):
    image = tf.io.read_file(path)
    image = tf.image.decode_jpeg(image,channels=3)
    image = tf.image.convert_image_dtype(image,tf.float32)
    image = tf.image.resize(image,size=[224,224])
    return image

# Create Batch data of numpy array return processImage(image), label
def pairData(image, label):
    return processImage(image), label

def batchData(image,label=None, for_valid=False, for_test=False):
    if for_test:
        data = tf.data.Dataset.from_tensor_slices((image))
        batch = data.map(processImage).batch(32)
        return batch
    elif for_valid:
        data = tf.data.Dataset.from_tensor_slices((tf.constant(image), tf.constant(label)))
        batch = data.map(pairData).batch(32)
        return batch
    else:
        data = tf.data.Dataset.from_tensor_slices ((tf.constant(image), tf.constant(label)))
        data = data.shuffle(buffer_size=len(image))
        batch = data.map(pairData).batch(32)
        return batch

unique_label = np.unique(y_test)
y_test = process_label(y_test)
y_train = process_label(y_train)

train_data = batchData(x_train,y_train)
valid_data = batchData(x_test,y_test, for_valid=True)

x_train,y_train,x_test,y_test = loading_data()
x_train, x_test
y_train, y_test

"""***LANGKAH 3 : MEMBANGUN NEURAL NETWORK***"""

# Create mode structure
import keras
import keras.utils
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from tensorflow.keras import optimizers
model3 = keras.Sequential([
    # Input layer
    keras.layers.Conv2D(input_shape=(224,224,3),filters=32,kernel_size=(3,3),activation='relu'),
    keras.layers.MaxPooling2D(),

    # Hidden Layer
    keras.layers.Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),activation='relu'),
    keras.layers.MaxPooling2D(),
    keras.layers.Conv2D(input_shape=(224,224,3),filters=128,kernel_size=(3,3),activation='relu'),
    keras.layers.MaxPooling2D(),
    keras.layers.Flatten(),
    keras.layers.Dense(128),
    keras.layers.Activation('relu'),

    # Output layer
    keras.layers.Dense(2),
    keras.layers.Activation('softmax')
])

# Compile model
model3.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(),metrics=['acc'])

model3.summary()

"""Model yang diberikan adalah sebuah Sequential model yang terdiri dari beberapa layer. Layer-layer tersebut adalah Conv2D, MaxPooling2D, Flatten, Dense, dan Activation.

~Conv2D adalah layer konvolusi 2 dimensi yang digunakan untuk memproses input gambar pada level fitur.

~MaxPooling2D adalah layer pooling 2 dimensi yang digunakan untuk mengurangi dimensi spasial dari representasi fitur sehingga mengurangi jumlah parameter dan mengontrol overfitting.

~Flatten adalah layer untuk melakukan penggabungan dari output layer sebelumnya sehingga menjadi input untuk layer Dense selanjutnya.

~Dense adalah layer fully connected dengan setiap neuron dihubungkan ke semua neuron pada layer sebelumnya dan setelahnya.

~Activation adalah layer aktivasi yang menambahkan non-linearitas ke dalam model dengan menggunakan fungsi aktivasi.

Total parameter model ini sebesar 11,169,218 yang semuanya dapat dipelajari selama proses training. Model ini terdiri dari 2 output neuron yang masing-masing merepresentasikan kemungkinan gambar yang diinputkan memiliki masker atau tidak.

***LANGKAH 4 : MODEL LATIHAN***
"""

# buat panggilan bail penghentian awal
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',patience=3)

# simpan riwayat model menjadi variabel
history3 = model3.fit(train_data,validation_data = valid_data,validation_freq=1,epochs=5,callbacks=[early_stopping],verbose = 1)

"""***LANGKAH 5 : PLOT HISTORY***"""

#plot model training history
def plot_history():
    plt.plot(history3.history['acc'],label='acc')
    plt.plot(history3.history['val_acc'],label='val_acc')
    plt.plot(history3.history['loss'],label='loss')
    plt.plot(history3.history['val_loss'],label='val_loss')
    plt.legend()
    plt.title('Training History')
    plt.xlabel('epoch')
    plt.ylabel('value')
    plt.tight_layout()
    plt.grid(True)
    plt.show()

plot_history()

"""***LANGKAH 6 : DEMONSTRASI***"""

import os
import matplotlib.pyplot as plt
def predict(unseen_image = []):
    test_data = batchData(unseen_image,for_test=True)
    prediction = model.predict(test_data)
    for image, pred in zip(unseen_image,prediction):
        fig, axes = plt.subplots(nrows=1,ncols=2)
        axes[0].imshow(processImage(image))
        axes[0].axis(False)
        axes[0].set_title('Actual Image')
        axes[1].bar([0,1],pred)
        axes[1].set_xticks([0,1])
        axes[1].set_xticklabels(['with_mask', 'without_mask'])
        axes[1].set_title('Prediction Probability')
        plt.savefig(f'/content/drive/MyDrive/output1/prediction_test_{image.split("/")[-1].split(".")[0]}.jpg')
        plt.show()

predict(list(map(lambda x : f'/content/drive/MyDrive/test_facemask/{x}',os.listdir('/content/drive/MyDrive/test_facemask'))))